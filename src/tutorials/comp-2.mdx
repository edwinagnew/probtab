
## What is Complexity Theory really?

In Level 1, we saw how computer science was born out of trying to understand the limits of what can be answered by these funny little machines called computers. In this level, we're going to examine the details of these computers a little more closely. The goal is the work up towards the formal definition of the $\mathsf{P}$ vs $\mathsf{NP}$ problem mentioned at the end of Level 1. To do so, you'll need to be comfortable with some basic notation from set theory. You should be able to understand what a statement like $a \in A \subseteq B \cap C$ means. If you can't, check out the little refresher below.

import { Accordion } from "react-bootstrap"

<Accordion >
      <Accordion.Item eventKey="0">
        <Accordion.Header>Baby Set Theory</Accordion.Header>
        <Accordion.Body>
          A *set* is just a well-defined way of talking about a collection of things. If I give you a set, the only questions worth asking are about what it contains. Because a set is a mathematical concept, there can be no ambiguity about whether or not a set contains something. 
          
          Sets are usually written in capital letters like $A, B, C$ and the things inside them are usually written in lowercase letters like $a, b, c$. If a thing $a$ is in a set $A$, it is called an *element* of that set and we write $a \in A$. A set can be defined by writing a list of its elements, e.g. $A = \{a, b, c\}$. Because we only care about what a set contains, the order and frequency of elements is irrelevant and so $\{a, b, c\} = \{c, b, a\} = \{a, a, a, b, b, c\}$. On the other hand $\{a, b, c\} \neq \{a, b\}$ because the latter does not contain $c$.

          Writing a set as a list of elements is only convenient when there are a small number of elements. However, because its a mathematical concept, sets very often contain *infinitely* many elements. This isn't as scary as it sounds: the set of natural numbers $\{0, 1, 2, ...\}$ is presumably already very familiar to you. It is so important that it is given a fancy symbol: $\mathbb{N}$. We can use sets to define others sets by giving a rule for picking out the elements. For example, $\{n \in \mathbb{N}: 0 < n \leq 10\}$ is the same of numbers between 1 and 10. The colon 
        </Accordion.Body>
      </Accordion.Item>
</Accordion>

Simplifcations:
- decision (yes/no) rather than answers
- single tape TM
- binary alphabet

### Turing Machines

Recall that the theoretical computer science is mostly based around playing with the capabilities of this funny imaginary contraption called a *Turing Machine* (TM). In level 1, we worked with an informal definition that basically treated a TM as a little cart whizzing up and down an infinitely long tape according to some pre-defined rules. The TM is given a particular question by writing some input on its tape and then after whizzing around for a while, spits out YES or NO (ideally). In order to probe further, we're going to need a slightly more precise definition. So let's just go for it. The technical definition of a Turing Machine is simply a tuple $(Q, \Sigma, I, q_0, F, \delta)$ where $I \subseteq Q, q_0 \in Q, \delta: Q \backslash F \times \Sigma \to Q \times \Sigma \times \{L, S, R\}$. Happy? No? Let's dive a little deeper:


* $Q$ - set of states. This is the basis of a Turing Machine's programming language and will become clearer with an example.
* $\Sigma$ - alphabet. This is the (finite) set of symbols that may be written on the tape at any point. It must always contain a blank symbol $\square$. Very often, it is assumed for simplicity that $\Sigma = \{0, 1, \square \}$ - called the *binary alphabet*. This contents and size of the alphabet makes no real difference to a TM's abilities because if you're given a TM with a larger alphabet, you can always imagine subsituting a symbol with its binary encoding and producing a TM with a binary alphabet that does exactly the same as the original one. Don't worry about the details of this. Just know you're safe to assume the simplest possibility.
* $I$ - input alphabet. This is the (finite) set of symbols that which the *input* may be written in. Clearly, $I \subseteq \Sigma$ but they may differ because $\Sigma$ contains extra symbols that are useful for intermediate working out. By similar reasoning to above, you can safely assume that $I = \{0, 1, \square\}$.
* $q_0$ - initial state. This is the where the TM begins. Will make more sense with an example.
* $F$ - final states. This the set of states (i.e. $F \subseteq Q$) corresponding to a completed calculation. Once a final state has been reached, whatever is on the tape is considered the output.
* $\delta$ - transition function. This is by far the most important part of a TM's definition. Given a current state it takes in the current tape symbol, spits a new symbol (to write on the tape), a new state and whether to move one position left or right along the tape (or stays in place). Thus it can be described as a function $\delta: Q\backslash F \times \Sigma \to Q \times \Sigma \times \{L, S, R\}$. In fact, it is often simpler to assume $\delta$ is a *partial* function which means it doesn't have to be defined on all pairs of states and symbols. $\delta$ can get very complicated but I will give a simple example below.

And that's it! There's quite a lot of book-keeping about what symbols and letters are allowed, but the interesting part of a TM is all contained in $\delta$. To help see how all these parts fit together, I will explicitly construct a TM that solves the following problem: replace all the 0's on the tape with 1's. This is not a particularly useful or impressive problem but who cares?

For simplicity, let's assume $\Sigma = I = \{0, 1, \square\}$. Now let's construct $\delta$. Intuively, what it will do is write a 1 regardless of the current symbol and then move right one position. So let's write that out:
- $\delta(q_0, 0) = q_0, 1, R$ - if we're in the starting state $q_0$ and read a 0, then stay in $q_0$, write a 1 and move right. 
- $\delta(q_0, 1) = q_0, 1, R$ - same but for reading a 1

Is that it? Let's work through an example to find out. Imagine the tape begins with $1010$. By definition, the TM begins in $q_0$. Here's a (very bad) drawing of the starting situation:
![Turing machine diagram](./imgs/tm-basic-1.jpg)

As described, there's a tape containing $1010$ (and then infinitely many blanks) and the head of a TM in state $q_0$ looking at the first tape position. So by definition of $\delta$, the first step should be to rewrite the 1, then move right and remain in state $q_0$. This gives:
![Turing machine diagram 2](./imgs/tm-basic-2.jpg)

Next we'll do the same, replacing the 0:
![Turing machine diagram 3](./imgs/tm-basic-3.jpg)

This will continue until arriving at the end of the tape:
![Turing machine diagram 5](./imgs/tm-basic-5.jpg)

What happens when reading a blank? We are yet to define $\delta(q_0, \square)$ and so the machine would crash. Instead, since we have succesfully arrived at the end of the input, the ideal behaviour would be to simply stop. Thus we define $\delta(q_0, \square) = q_f, \square, S$. Since $q_f$ is intended to be a final state, we must remeber to include $q_f \in F$. Having done so, we get:
![Turing machine diagram 6](./imgs/tm-basic-6.jpg)

Great success! A compact way of representing the behaviour of this TM is the following "state machine diagram" which I drew using [this website](https://turingmachine.io/):
![Turing state machine diagram](./imgs/tm-sm.png)



### Algorithms

Take a look back at the Turing Machine we just defined but imagine the tape began with $1010101101110101011$, rather than $1010$. Would our little contraption still be able to replace it with all $1$'s? The answer - which should be pathetically obvious - is yes! In fact, even though we designed the machine with only $1010$ in mind, it is capable of rewriting absolutely *any* sequence of $0$'s and $1$'s. Though some of these sequences may be too large to ever be written down in the lifetime of the universe, our machine can *in theory* still handle them. 

I think this is absolutely remarkable. Some finite contraption, produced by my finite mind, is capable of solving infinitely many things. In our example, the problem was an extremely uninteresting one. But that was just for illustration. Imagine we want to add two numbers. Suppose that the numbers are written in *unary* - the number $n$ is simply $1$ written $n$ times - and the input is always given as $n$ ones, followed by a $0$, followed by $m$ ones. To compute $n$ plus $m$ we need to rewrite the tape to contain $n + m$ ones. So here's a sketch of how a TM could solve it: rewrite all the symbols with a $1$ until reaching the first blank (as we did before). Then move left one step and replace the final $1$ with a blank and then finish. I'll let you think about how to modify our old TM to do this. You should also think about why this is a succesful solution to our problem (hint: how many $1$s will be left at the end?)

Once again we have managed to construct a very feeble looking contraption which is nevertheless capable of adding *all* numbers (in this idiosyncratic unary form). The reason this keeps happening is because this is what computers are all about! More precisely, we have discovered an *algorithm* for each of our problems. An algorithm is a list of simple instructions which, when followed, solves a certain problem. Or, in the wonderful words of Kleene "an algorithm is a *finite* answer to an *infinite* question". This sounds like it should be impossible - how could we even comprehened infinite questions, let alone give finite answers? The answer: computers (i.e. the means for executing algorithms)! I like to think of computers as periscopes through the roof of Plato's cave, enabling us to glimpse the infinite, mathematical truth lurking above. In case that somehow floats your boat, here's a bizzare AI generated depiction (the blue periscope is the computer, the sailor is the computer scientist):

![Turing state machine diagram](./imgs/platonic_periscope.png)


You might be wondering, what's the point of a computer being able to give infinitely many different answers? We live in a finite universe so are only ever capable of extracting a finite slice of a computer's knowledge (yes [I mean knowledge](/bonus)). One response is that Turing Machines come from *theoretical* computer science and so the whole point is to study the *theoretical* limit of computers. But I think the more interesting response is that only being able to give a handful of specific answers often feels suspiciously like cheating. Imagine I sold you a calculator but all it could tell you was $2+2=4$. This is indeed a true fact about arithmetic, yet whatever I sold you clearly is not *calculating* anything - it has simply stored the answer like an inanimate post-it note. It wouldn't change much if it could also tell you $213412 + 89532 = 302944$ (ironically I had to a computer to tell me that). A similar sentiment is expressed in this [clip from Modern Family](https://www.youtube.com/shorts/MHzvbvgqHO4). The point is that a computer is only *computing* something when it has to wait for you to give it an input. Moreover, if there were only finitely many different inputs you could ever ask it, then the computer could simply memorise all the individual answers to each of your possible questions. It is only when there are infinitely many different possible inputs that we can be sure a computer is truly computing the answer *on the fly*. 


Deciders?

Defn on language


### Universality

### Limits 

### Efficiency

Layers of understanding:
- right answer
- correct program
- proof of correctness of program
- proof of optimality of program
- smallest proof of optimality of program
- ...

### Regular Languages