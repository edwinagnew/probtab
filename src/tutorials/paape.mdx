## Pragmatic Armchair Epistemology

## Algorithmic Epistemology?

Imagine sitting comfortably in a deep armchair. You're sat by a crackling fireplace with a pipe in your mouth, philosophising away. You're wondering about wondering about all sorts of wonderful things. You're curious and fascinated by these concepts and thoughts but there's one problem. The thing is that you can't be bothered to get out of your armchair something something

...

The goal of this essay is to explore some surprising connections I've noticed between computability theory and epistemology. The main idea is that the definition of an algorithm accidentally provides a pretty interesting definition of knowledge - not of the real world but knowledge of the *a priori*. 

Let's break it down. Computability theory is about Turing machines and the limits of algorithms. Epistemology is the study of knowledge. *A priori* epistemology is, therefore, the study of knowledge without experience. What's the connection between these two? I imagine there's already alarm bells going off in your head. "Here comes another computer scientist claiming that everything is a computer". Except that's not what I'm claiming! Ironically, we'll see later that traditional epistemology actually assumes we're more like computers than I am. 


### Epistemology

Before we start, I would like to give a brief hommage to knowledge. I think knowledge is fantastic. I am frequently blown away by how much knowledge we as humans have acquired about the world. Some of it is extremely useful - knowing about microorganisms has enabled us to save millions from disease. Some of it is extremely dangerous - knowing about atomic nuclei has enabled us to create nuclear weapons capable of killing billions. Some of it is utterly useless - knowing the first 2 million digits of $\pi$ has allowed us to... But all of it is marvellous. So marvellous I think its worth taking a little step back and asking: what exactly is all this knowledge stuff? Where does it come from? Why are we so good at acquiring it? Wait a second - are we actually any good at acquiring it? What if we're wrong? Oh shit - do we even know anything at all? 

There's a famous quote that says something like "the entire history of Western philosophy can be summarised as a series of footnotes to Plato". Epistemology is probably one of the best examples of this because one day, all the way back in Ancient Greece, Plato was comfortably reclining on an ancient Greek sofa and decided that *knowledge* could be defined as *justified, true belief*. I'll refer to this idea as JTB. In the intervening 2000 years, there has been a tremendous amount of bickering about this definition and not a huge amount of modifications to it. So what are these justified true beliefs? 

The first thing to say is that you probably have a lot of very different connotations of the word "knowledge" and not all of them seem to be directly related to special beliefs. For example, you might think of books or science or google as being the epitome of knowledge. And that's completely fair. So it's worth clarifying that when philosophers talk of defining knowledge, they are usally talking about the questions of the form "what does it mean to say that Alice *knows* that Socrates is mortal" (this is particularly famous example so let's pretend this is what Plato was talking about).  So let's break down, in reverse order.


#### Belief

First things first, for Alice to know something it seems pretty important that she at least believe it. It would be pretty weird for us to credit Alice for knowing something she did not even believe. There's a whole tangent we could take here about what exactly it means for Alice to believe something and all sorts of horrible edge cases to deal with (e.g. whether Alice needs to be certain or whether Alice needs to be consciously aware of her beliefs etc etc). But you should hopefully have a strong intution that belief is necessary for knowledge. For an operational definitin, imagine Alice has been given a test of true or false questions and has decided to tick true for the statement "Socrates is mortal". 

#### Truth

If you've even spoken to anybody, you'll probably have noticed that people have all sorts of beliefs about all sorts of things. And that's great - beliefs are useful for all sorts of things. Good job. But some beliefs are better are than others for the very simple reason that they are wrong. For example, people used to believe that the Earth is flat. This belief was fundamental to astronomical modelling, nautical adventuring and a fair amount of anthropocentrism. If you'd asked these people they would have said that they *knew* that the Earth is flat. However, they didn't. It is now clear to us that they cannot have known this because it turns out that the Earth is *not* flat. However hard you try, you cannot know something that is false. Of course, there are all sorts of reasons you might be interested in false beliefs. But that would make you an agnotologist (fun fact: the study of ignorance is called [agnotology](https://en.wikipedia.org/wiki/Agnotology)). And right now, we're talking about epistemology.

So to know something it has to be true. This sounds fairly unproblematic. There are, of course, countless bizarre edge cases epistemologists have bickered about (for example - are future events true? What about counter-factuals? What about statements about works of fiction?) but I won't bother with those either.


#### Justification

It's tempting to think that we're done here: surely if Alice beliefs something to be true and it is in fact true, then she knows it? Not quite, according to conventional epistemology. Consider the following example: Alice wakes up one morning with a bad feeling in her bones and decides it's going to rain today. Later that day, it does in fact rain. Did Alice know about the rain? Or perhaps Alice's hair is wet and she concludes that it's raining. Imagine that it is in fact raining, but she was inside a dark room and her hair was wet because she just got out of the shower.


Hopefully these examples make you feel a little uneasy. You'll notice that both cases constitute true knowledge yet unconvinced that Alice *really* knew about the rain. Although she was right, it seems that she was right merely by luck. And there's something about lucky guesses that doesn't feel like they deserve the full status of knowledge. So to have knowledge, say the epistemologists, Alice must not only be right but she must be right for the *right reasons*. They call this justification.

Justification is by far the stickiest part of JTB. This is where most of the bickering happens and has produced the most competing alternatives. But what underlies them all is there needs to be a story of where Alice's (true) belief came from and some specific criteria for this story being a good one. In the examples I just gave, the idea was that neither bad-feeling-in-bones nor wet-because-shower were very good reasons for predicting the rain. On the other hand, Alice seeing dark clouds overhead and hearing thunder in the distance are pretty good reasons for believing there may be a spot of rain. I often imagine justification as follows: after filling out the true/false questionnaire, Alice is audited by the council of epistemologists. For each of the statements she answered correctly, they submit her to brutal questioning and only if she passes their standards do they award her the certificate of knowledge. This might sound like the sort of nit-picking that everyone except philosophers finds completely pointless. So allow me to justify the condition of justification a little further (haha), in a few different ways (feel free to skip if you're not interested):

1. My primary school maths teacher always used to bang on about showing my working for my answers. They weren't interested in the right answer, but the right justification. I think they were worried I was copying off my mates or something. So there are at least some situations where the epistemic journey is more important than the destination.

2. As I mentioned at the beginning, knowledge is a pretty neat thing to have - whether because it's useful or just because its kind of cool. So it would be nice to have more of it. So a well explained account of the *process* of acquiring knowledge might help us get better at getting more of it. (unfortunately, this probably also requires epistemologists coming to agreement about what justification is. So far, the footnotes have not converged on this matter)

3. One especially nifty property of knowledge is that it can be shared. I think this is quite a crucial factor in the rapid pace of development of human civilisation relative to the pace of evolution. Anyway, think of a belief that you have that you would have a hard time convincing others. For me, it might be "pasta and pesto is the best meal to make in a jiffy". I really do believe this and yet I've never been able to convince any of my friends of this. After some painful reflection, I've decided this is because I am not able to communicate a good justification for this belief. On the other hand, in the imaginary case I ever get into an argument about whether $2+3=5$, I am confident that whipping out a calculator should settle the dispute without difficulty. This is because calculators are something that we all have access to. My feelings about pesto are not. Though many philosophers would probably disagree with me about this, I reckon that I am unable to access a lot of what goes on in my head, let alone find a coherent way of describing it to you. Yet there are some cases where I am able to take a belief of my own and copy it into your brain (and vice versa - although I guess you'd have to write a rambling blog post and send it to me somehow or something). All jokes aside, the reason I feel obliged to justify the justification condition is I don't expect you to take me on faith that it matters but I do hope that you'll see where I'm coming from after these examples.

So that's some of my justification for the justification condition (sorry if this is all too meta). Please be aware that if you asked two other epistemologists about this, you'd get at least seven reasons they disagree. In the jargon, you'd maybe call my position a social externalism just to show how niche all of this gets. Anyway, if you want to know what the other knowledge nerds think about justification then fuck off and find out yourself (you could try reading [all of this](https://plato.stanford.edu/entries/knowledge-analysis/) - I certainly haven't). To elaborate a little on the controversy, there are two main camps in the bickering about justifcation. I will explain each in the metaphor of the council of epsitemologists:

- *internalism* requires that Alice has some sort of access to her justification or at least that all of the justifcation process occurs *inside* her brain. In other words, Alice must be able to defend herself in front of the council. One particularly optimistic flavour of this is called foundationalism which requires that Alice derive her knowledge from a handful of undoubtable beliefs. So her defense for the mortality of socrates might go something like "your honour, it is part of our consitution that I am justified in believing that all men are mortal. Moreover, it was established in the renowned case of Plato v. Descartes 1700 that I am justified in believing that Socrates was a man. Since the laws of logic are written in our consitution I must be justified in believing that Socrates is mortal".
- *externalism* says that justification includes factors outside of Alice's brain. So Alice does not have to represent herself in front of the council, may be represented by a well-prepared yet impartial legal team. My favourite flavour of externalism is called reliablism which requires that the process of belief formation be generally reliable for that sort of thing (obviously there more precise ways of spelling this out). So in the case of will it rain, Alice's legal team might offer the following defense: "your honour, a psychological evaluation found that Alice's rain belief was formed because she saw the rain clouds. As this graph demonstrates, witnessing rain clouds correctly predicts rain in more than 99% of cases. Thus we deem Alice's belief to be well justified"

As the length of this section relative to the other two shows, justification is a whole mess. Well guess what. It gets worse!

#### More???

Consider the following situation. Alice is sitting at home. Her clock reads 3pm so she concludes that it is 3pm. It just so happens that it really is 3pm. It also just so happens that Chris the esteemed clock servicer had come that very morning to check on the status of the clock. But there's a catch: the clock is broken. Does this count as knowledge? Alice has a true belief that it is 3pm. Her belief seems justified because the clock had just been serviced - if there were was ever a time to trust the reading of the clock it would be now. So on the one hand, it meets the JTB criteria. Yet on the other hand, something should feel off. This feels like another case of being right by getting lucky. Even though there was justification, it somehow didn't quite match up to what should have happened.

This type of bizarre edge case is called an Gettier case, named after the author of one of the more dramatic epistemological footnotes. Gettier cases are all situations where there's a justified true belief that just doesn't feel like knowledge. Here's another one: Alice is on a walk and sees a hill in the distance. On the top of the hill is a cardboard model of a sheep. From Alice's point of view it looks like a real sheep, so she concludes "there's a sheep on that hill". It just so happens that behind the cardboard cut out there is in fact a real sheep. So her belief is incidentally true (and seems justified). But once again there's something wrong: the sheep that's there is not the one she thought she was forming a belief about. 

By the way, you'll notice that the sheep case relies on Alice generalising from "that thing on the hill looks like a sheep" to "there is a sheep on that hill". It's only the latter statement which is dodgy because that's the one referring to the unseen real sheep. The assumption that beliefs automatically generalise like that is something that's usually swept under the rug. On the one hand, it seems quite reasonable to expect that if Alice is commited to some fact, then she'll need to be comitted to any other fact that immediatlely follows from it. For example, it seems strange for Alice to agree that Socrates is man but disagree that Socrates is mortal. The philosophy jargon name for this is called logical omniscience because it means you know all the logic consequences of your beliefs. For situations like seeing things on hills, this is usually pretty handy and unproblematic. But one issue is this means that we all already know all of maths (including all the unsolved problems) as long we know enough maths to count. This is what I was referring to when I mentioned that classical epistemology assumes we're closer to computers than I will.

Gettier cases kicked off a flurry of footnotes trying to formulate a fourth condition that would rule out these dodgy yet justified true beliefs being accredited knowledge. One issue in the sheep case is that whether or not the real sheep was present, the cardboard sheep would still have led Alice to believe there was a sheep there. A generalisation of this has been proposed as a fourth condition for knowledge. Unforuntunately, each one of these new possible conditions has quite quickly been met with a fresh Getter-like case of peculiar accidental justifications and incidental truths that seems to knock these "JTB+X" proposals down. 

I won't bother delving any further into this rabbit hole. The point is that JTB has come under heavy attack recently and not done very well to recover. 

Now for something completely different.

### Algorithms

Now its time for me to argue that algorithms have something to do with knowledge. Informally, an algorithm is a list of instructions for how to do something. Formally, an algorithm for some problem $L$ is a Turing machine $M$ such that $M(x) = 1 \iff x \in L$ (read the tutorials if that doesnt mean anything to you). I claim that such an algorithm is sufficient for $M$ knowing $x \in L$. In other words, a computer "knows" something if it used an algorithm to solve it.

Firstly, how does this compare to JTB? Belief clearly corresponds to the computer saying yes, i.e. $M(x) = 1$. I kind of snuck this in with the metaphor of answering a true/false questionnaire. Truth clearly corresponds to $x \in L$, i.e. getting the right answer for $x$. So $M$'s beliefs are true because $M(x) = 1 \implies x \in L$. What about justification? The crux of my argument is that an algorithm's justification is the reverse direction $x \in L \implies M(x) = 1$, i.e. the fact $M$ is capable of correctly answering *all* instances of an entire language. As discussed in the tutorials, the computer scientist's reason for defining algorithms relative to languages rather than individual strings seems to be about avoiding the issue of merely regurgigating the correct answer. By making sure non-trivial problems contain infinitely many strings, a turing machine is forced to *do* something to work out an answer. And that process of *doing*, which is what computer science is all about, seems like an interesting concept of justification.

The idea that a computer must be able to solve all problems of a similar type is very reminiscent of reliabilism. Recall that reliabilism is the idea that a belief is justified if it is formed by a generally reliable process. In fact, I think the only difference is that reliabilism usally talks about knowledge of the real world with the relevant processes being perception, memory, etc whilst algorithmic justification talks exclusively about *a priori* knowledge of numbers, graphs, formulae etc. One immediate benefit of this retreat to abstraction is that it solves reliabilism's problem of generality: when we say my vision is reliable, does that include at night, or in a dark room, or when Alice is hallucinating or when someone is shining lasers in her eyes? For computability theory, the scope of a algorithmic process is precisely defined by its language, i.e. $L(M) = L$. 

Another sticky part of reliabilism this helps address is clarifying what exactly Alice is supposed to be reliable at. Imagine Alice buys a lottery ticket and because there's only a 1 in a million chance of winning concludes she won't win. Is this not a 99.9999% reliable process? NO! Recall that a probablistic algorithm $M$ is a probabalistic Turing machine where $x \in L \implies Prob(M(x) = 1) > 2/3$. Then rejecting every lottery ticket is not a good probabilistic algorithm because if $x \in L$ ($x$ is a winning ticket) then the probably of accepting $x$ is zero! This would be a like an "algorithm" for deciding that a number is prime that just always says no. The problem from a subtle logical point called the order of quantifiers. There's an enormous difference between "getting 90% of the answers right" and "for a particular question, theres a 90% chance of getting it right". In the first case, asking several times will always give you the same answer (this is a deterministic algorithm). Whereas in the second case, asking repeatedly means the overall accuracy improves because you can take majority vote option. Lets go back to vision. Are your eyes reliable because they work in 50% of lighting conditions or because in any lighting condition, there's a 5

#### What does this have to do with Alice?

Alright so maybe the computer scientists definition of an algorithm happens to accidentally resemble the reliabilist's definition of justification. But what is this actually telling me?

Well the first relevant message from the computer science nerds is that there are some true beliefs that cannot be justified. So there are things that provably cannot be known. Let's consider an example: can we know whether a program will do what it says on the tin? By Rice's theorem, there is no algorithm for this. So while we may be able to hand pick a couple of examples, a computer can never be able to tell in general. CUT? --> Suppose that nevertheless there was a person who was able to. Imagine that for every program they'd ever glanced at, they were correctly able to say whatever it was they did. First of all, they'd never be able to effectively explain how they did it. Nor could a perceptive observer - even one equipped with an elaborate brianscanner - ever be able to "read off" this prophet's thought process. In both cases, either the extracted process is written down ambiguosly or it isn't. If its written down ambiguously, then noone else will be able to learn how to perform the same trick and so this "knowledge" is effectively useless. Or it's written down unambigously which means it could be translated into an algorithm which contradicts the fact there's no algorithm for it. This is what the CT thesis says: a computer can follow any unambgious formal procedure. That doesn't mean we have to think like computers. But it means we have to communicate this type of knowledge as if we did. 

So if a computer can't know something, we presumably can't either. What about the other direction? Surely we can't know everything that computers can? Well yes we can - if you have an algorithm and functioning computer for something then just plug in whatever instance you care about. There's some pragmatic concerns like the size of your computer or the length of your patience - but this is exactly what is adressed by the space and time classes of complexity theory! Nondeterminstic classes cover interesting concepts as well. There's a famous socratic dialogue where Socrates asks a random slave in the market a bunch of leading questions to trick him into deriving some theorem from geometry. His point is to convince some other random dude in the market that the first random dude had implicit knowledge of geometry. Socrate's trick was to use a nondeterministic algorithm: the slave was able to get the answer through by verifying the hidden suggestions Socrates was making. If you think this is cheating and shouldn't count as knowledge then you'd better hope that $P\neq NP$! 

??There is one juicy edge case of this which is where you have an algorithm that is correct but the *proof* of the correctness of that algorithm is uncomputable. I'm not sure I know of any examples of this but it seems plausible. 

Efficent alg -> compression -> AIT

#### Ok but what about Gettier cases?

The general structure of a Gettier case is the formation of a justified false belief, followed by a twist that makes that belief "accidentally" true. The closest I can come to creating an algorithmic Gettier case is something like the following: an algorithm which expects its input to be the encoding of a graph is instead fed the encoding of a sudoku instance. The sudoku instance is nevertheless interpreted as a graph and the algorithm proceeds outputting $1$. It just so happens that $1$ is the correct answer for the soduko problem. So this algorithm correct? There are two responses:
1. This accidental interpretation of a suduko instance as a graph is specific to the $x$ that was fed in. In which case, the graph algorithm will not reliably solve soduko instances and so the algorithm is not correct.
2. It just so happens that all sudoku instances can be intrepreted as graphs and that the graph algorithm will always give the right response for sodukos. In which case, the algorithm is correct so the answer is yes.

By the way, the second option happens remarkbly often in complexity theory and is known as a reduction. It is one of the most powerful tools complexity theorists have for establishing lower bounds and connecting the complexity of very different seeming problems. So it is not an undesirable bug of "accidental" misinterpretation, but an extremely powerful feature of computational expressiblity.

